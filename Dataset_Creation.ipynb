{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WI7LvxFS829r"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","from glob import glob\n","\n","import IPython.display as ipd\n","from tqdm.notebook import tqdm\n","\n","import subprocess\n","\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","import os\n","from PIL import Image as PImage\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","import random\n","from pprint import pprint\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_IOInmcp86_M","outputId":"83752943-9993-48a8-f150-744f10581b6d","executionInfo":{"status":"ok","timestamp":1673902891969,"user_tz":-60,"elapsed":24562,"user":{"displayName":"marco nacca","userId":"06456378347446857363"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"         # default location for the drive\n","\n","drive.mount(ROOT)              # we mount the google drive at /content/drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u2unV3uM884Q"},"outputs":[],"source":["directory = '/content/drive/MyDrive/VISIOPE_PROJECT/dataset/c23/videos' "]},{"cell_type":"markdown","source":["# Usefull Function"],"metadata":{"id":"seW83CNonAJH"}},{"cell_type":"markdown","source":["` display_cv2_img`  is used to show the frame of the video"],"metadata":{"id":"kBzPq2Pcm1KP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7W-LVZV3AB2R"},"outputs":[],"source":["def display_cv2_img(img, figsize=(10, 10)):\n","  img_=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","  fig, ax=plt.subplots(figsize=figsize)\n","  ax.imshow(img_)\n","  ax.axis(\"off\")"]},{"cell_type":"markdown","source":["` Show_video`  is used to see the video on Colab"],"metadata":{"id":"2PGNKfeamsS-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZbFW_aTf-47u"},"outputs":[],"source":["cap = cv2.VideoCapture('/content/drive/MyDrive/VISIOPE_PROJECT/dataset/c23/videos/000.mp4')\n","ret, img = cap.read()\n","display_cv2_img(img)\n","\n"," def show_video(video_path, video_width = 600):\n","  video_file = open(video_path, \"r+b\").read()\n","  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n","  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n","  \n","show_video('/content/drive/MyDrive/VISIOPE_PROJECT/dataset/c23/videos/000.mp4')"]},{"cell_type":"markdown","source":["# Function for split video in frame"],"metadata":{"id":"8nlGzzBaoJph"}},{"cell_type":"markdown","source":["With the following functions, we split the videos in frames and we take as dataset only the first frame of all the videos."],"metadata":{"id":"AvL1G330tI0u"}},{"cell_type":"markdown","metadata":{"id":"R6FmPXQQiTuP"},"source":["##Save frame of original video in data_original (already done)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qX9YdY0H9kWf"},"outputs":[],"source":["directory = '/content/drive/MyDrive/VISIOPE_PROJECT/dataset/c23/videos' \n","for filename in os.listdir(directory):\n","  image_name = filename.replace('.mp4', '')\n","  file_path = directory + '/' + filename\n","  #print(file_path)\n","  cap = cv2.VideoCapture(file_path)\n","  ret, img = cap.read()\n","  #show_video(file_path)\n","  cv2.imwrite('/content/drive/MyDrive/VISIOPE_PROJECT/data_original' + '/' + image_name + '.jpg' , img)\n","  "]},{"cell_type":"markdown","metadata":{"id":"_zwuVbhUifIw"},"source":["##Save frame of Deepfake video in data_deepfake (already done)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-_bGsAVEQHC"},"outputs":[],"source":["directory_deepfake = '/content/drive/MyDrive/VISIOPE_PROJECT/dataset/Deepfakes/c23/videos'\n","for filename in os.listdir(directory_deepfake):\n","  image_name = filename.replace('.mp4', '')\n","  file_path = directory_deepfake + '/' + filename\n","  cap = cv2.VideoCapture(file_path)\n","  ret, img = cap.read()\n","  #print(img)\n","  #show_video(file_path)\n","  break\n","  cv2.imwrite('/content/drive/MyDrive/VISIOPE_PROJECT/data_deepfake' + '/' + image_name + '.jpg' , img)"]},{"cell_type":"markdown","metadata":{"id":"JE-ozE6XXctt"},"source":["##Save frame of Face2Face video in data_face2face (already done)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsjvKgXAbfdN"},"outputs":[],"source":["directory_face2face = '/content/drive/MyDrive/VISIOPE_PROJECT/dataset/Face2Face/c23/videos'\n","for filename in os.listdir(directory_face2face):\n","  image_name = filename.replace('.mp4', '')\n","  file_path = directory_face2face + '/' + filename\n","  cap = cv2.VideoCapture(file_path)\n","  ret, img = cap.read()\n","  #print(img)\n","  #show_video(file_path)\n","  cv2.imwrite('/content/drive/MyDrive/VISIOPE_PROJECT/data_face2face' + '/' + image_name + '.jpg' , img)"]},{"cell_type":"markdown","metadata":{"id":"TgmOZn0TXh-b"},"source":["##Save frame of neuralTextures video in data_neuraltexture (already done)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GczQ39rtXhYE"},"outputs":[],"source":["directory_neuraltexture = '/content/drive/MyDrive/VISIOPE_PROJECT/dataset/NeuralTextures/c23/videos'\n","for filename in os.listdir(directory_neuraltexture):\n","  image_name = filename.replace('.mp4', '')\n","  file_path = directory_neuraltexture + '/' + filename\n","  cap = cv2.VideoCapture(file_path)\n","  ret, img = cap.read()\n","  #print(img)\n","  #show_video(file_path)\n","  cv2.imwrite('/content/drive/MyDrive/VISIOPE_PROJECT/data_neuraltexture' + '/' + image_name + '.jpg' , img)"]},{"cell_type":"markdown","metadata":{"id":"1dqfssrSX7II"},"source":["##Save frame of FaceSwap video in data_faceswap (already done)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHJ5bXApYBh-"},"outputs":[],"source":["directory_faceswap = '/content/drive/MyDrive/VISIOPE_PROJECT/dataset/FaceSwap/c23/videos'\n","for filename in os.listdir(directory_faceswap):\n","  image_name = filename.replace('.mp4', '')\n","  file_path = directory_faceswap + '/' + filename\n","  cap = cv2.VideoCapture(file_path)\n","  ret, img = cap.read()\n","  #print(img)\n","  #show_video(file_path)\n","  cv2.imwrite('/content/drive/MyDrive/VISIOPE_PROJECT/data_faceswap' + '/' + image_name + '.jpg' , img)"]},{"cell_type":"markdown","source":["# FACE CROPPED"],"metadata":{"id":"WIoHgX9pMDws"}},{"cell_type":"markdown","source":["With the `cropped_image`, we take only the aerea of the images corresponding to the faces. The images that we will used for train and evaluate our model are composed of only the faces of the different people in the videos."],"metadata":{"id":"lhTNWji8oayN"}},{"cell_type":"code","source":["from re import I\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from PIL import Image\n","\n","def cropped_image(directory):\n","  # Read the input image\n","  img = cv2.imread(directory)\n","    \n","  # Convert into grayscale\n","  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","  # Load the cascade\n","  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n","    \n","  # Detect faces\n","  faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=[102,102])\n","  l1 = []\n","  # Draw rectangle around the faces and crop the faces\n","  for (x, y, w, h) in faces:\n","      #cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 256), 2)\n","      #faces = img[y:y + h, x:x + w]\n","      l1.append({'x': x, 'y': y, 'width':w, 'height': h})\n","      #cv2_imshow(faces)\n","  aerea=[]\n","  for elem in l1:\n","    help=elem['width']*elem['height']\n","    aerea.append(help)\n","  if len(aerea)!=0:\n","    m=max(aerea)\n","    ind=aerea.index(m)\n","    faces = img[l1[ind]['y']:l1[ind]['y'] + l1[ind]['height'], l1[ind]['x']:l1[ind]['x'] + l1[ind]['width']]\n","    #cv2_imshow(faces)\n","    #cv2.imwrite('face.jpg', faces)\n","  \n","  # Display the output\n","  #cv2.imwrite('detcted.jpg', img)\n","  #cv2_imshow(img)\n","  #cv2.waitKey()\n","  if len(faces)==0:\n","      return img\n","  else:\n","    return faces"],"metadata":{"id":"cVM48GGMT3rY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory = '/content/drive/MyDrive/VISIOPE_PROJECT/DATAFRAME/data_original'\n","for filename in sorted(os.listdir(directory)):\n","  file_path = directory + '/' + filename\n","  img_crop = cropped_image(file_path)\n","  #show_video(file_path)\n","  cv2.imwrite('/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_original_CROP' + '/' + filename , img_crop)\n"],"metadata":{"id":"4-1So-X_U8Ng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory = '/content/drive/MyDrive/VISIOPE_PROJECT/DATAFRAME/data_deepfake'\n","for filename in sorted(os.listdir(directory)):\n","  file_path = directory + '/' + filename\n","  img_crop = cropped_image(file_path)\n","  #show_video(file_path)\n","  cv2.imwrite('/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_deepfake_CROP' + '/' + filename , img_crop)\n"],"metadata":{"id":"230BPnWm2uOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory = '/content/drive/MyDrive/VISIOPE_PROJECT/DATAFRAME/data_neuraltexture'\n","for filename in sorted(os.listdir(directory)):\n","  file_path = directory + '/' + filename\n","  img_crop = cropped_image(file_path)\n","  #show_video(file_path)\n","  cv2.imwrite('/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_neuraltexture_CROP' + '/' + filename , img_crop)\n"],"metadata":{"id":"rMlHJvg7DPsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory = '/content/drive/MyDrive/VISIOPE_PROJECT/DATAFRAME/data_face2face'\n","for filename in sorted(os.listdir(directory)):\n","  file_path = directory + '/' + filename\n","  img_crop = cropped_image(file_path)\n","  #show_video(file_path)\n","  cv2.imwrite('/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_face2face_CROP' + '/' + filename , img_crop)\n"],"metadata":{"id":"ylZfefwTDo8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory = '/content/drive/MyDrive/VISIOPE_PROJECT/DATAFRAME/data_faceswap'\n","for filename in sorted(os.listdir(directory)):\n","  file_path = directory + '/' + filename\n","  img_crop = cropped_image(file_path)\n","  #show_video(file_path)\n","  cv2.imwrite('/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_faceswap_CROP' + '/' + filename , img_crop)"],"metadata":{"id":"yrZo6UndX0QD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Verify dimension Crop Folder"],"metadata":{"id":"tzm15BvW25Mu"}},{"cell_type":"code","source":["directory = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_original_CROP'\n","l=[]\n","for filename in sorted(os.listdir(directory)):\n","  l.append(filename)\n","print('Dimension of original file:')\n","print(len(l))\n","\n","directory = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_deepfake_CROP'\n","l=[]\n","for filename in sorted(os.listdir(directory)):\n","  l.append(filename)\n","print('Dimension of deep fake file:')\n","print(len(l))\n","\n","directory = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_face2face_CROP'\n","l=[]\n","for filename in sorted(os.listdir(directory)):\n","  l.append(filename)\n","print('Dimension of face2face file:')\n","print(len(l))\n","\n","directory = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_faceswap_CROP'\n","l=[]\n","for filename in sorted(os.listdir(directory)):\n","  l.append(filename)\n","print('Dimension of faceswap file:')\n","print(len(l))"],"metadata":{"id":"wrI1nD6i3Cys"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V5dNTgsP0BKL"},"source":["# Divide in Train and Validation and Test dataset\n"]},{"cell_type":"markdown","source":["With the following function, we go to create 4 folders,one for each manipulation *Deeep Learning*, *Face2Face*, *FaceSwap* and *Neural Texture*. Each of this are composed in the following way:\n","\n","\n","*   Training folder--> it is composed of 1600 images (800 images real and 800 images fake based on the manipulation considered)\n","*   Valid folder--> it is composed of 200 images (100 real and 100 fake)\n","*  Test folder--> it is composed of 200 images (100 real and 100 fake)\n","\n"],"metadata":{"id":"28OHDt-1o-c2"}},{"cell_type":"markdown","source":["## Original:"],"metadata":{"id":"WKlNw3Vtb3_9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1AgKHnsMJH-"},"outputs":[],"source":["count = 0\n","data_original = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_original_CROP'\n","for filename in os.listdir(data_original):\n","\n","  file_path = data_original + '/' + filename\n","  img = cv2.imread(file_path)\n","  if count < 800:\n","\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_deepfake/train/real/') \n","    cv2.imwrite(filename, img)\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_face2face/train/real/')\n","    cv2.imwrite(filename, img)\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_faceswap/train/real/')\n","    cv2.imwrite(filename, img)\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_neuraltexture/train/real/')\n","    cv2.imwrite(filename, img)\n","    print(\"train: \",count)\n","\n","  if count >= 800 and count < 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_deepfake/val/real/')\n","    cv2.imwrite(filename, img)\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_face2face/val/real/')\n","    cv2.imwrite(filename, img)\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_faceswap/val/real/')\n","    cv2.imwrite(filename, img)\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_neuraltexture/val/real/')\n","    cv2.imwrite(filename, img)\n","    print(\"val: \",count)\n","\n","  if count >= 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/Dataset_test/original/')\n","    cv2.imwrite(filename, img)\n","    print(\"test: \", count)\n","  count += 1"]},{"cell_type":"markdown","metadata":{"id":"U0BzHc-Mprs3"},"source":["## DeepFake:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aSULHmgI1T-y"},"outputs":[],"source":["count = 0\n","deep_fake = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_deepfake_CROP'\n","for filename in os.listdir(deep_fake):\n","  file_path = deep_fake + '/' + filename\n","  img = cv2.imread(file_path)\n","  if count < 800:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_deepfake/train/fake/') \n","    cv2.imwrite(filename, img)\n","  if count >= 800 and count < 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_deepfake/val/fake/')\n","    cv2.imwrite(filename, img)\n","  if count >= 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/Dataset_test/deepfake/')\n","    cv2.imwrite(filename, img)  \n","\n","  count += 1"]},{"cell_type":"markdown","metadata":{"id":"QrommPa-pvWo"},"source":["## Face2Face"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86oiznB2pckv"},"outputs":[],"source":["count = 0\n","face2face = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_face2face_CROP'\n","for filename in os.listdir(face2face):\n","  file_path = face2face + '/' + filename\n","  img = cv2.imread(file_path)\n","  if count < 800:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_face2face/train/fake/') \n","    cv2.imwrite(filename, img)\n","  if count >= 800 and count < 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_face2face/val/fake/')\n","    cv2.imwrite(filename, img)\n","  if count >= 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/Dataset_test/face2face/')\n","    cv2.imwrite(filename, img)   \n","  count += 1"]},{"cell_type":"markdown","metadata":{"id":"5AXMSKr4pxpV"},"source":["## FaceSwap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIEeJmYGpccb"},"outputs":[],"source":["count = 0\n","deep_fake = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_faceswap_CROP'\n","for filename in os.listdir(deep_fake):\n","  file_path = deep_fake + '/' + filename\n","  img = cv2.imread(file_path)\n","  if count < 800:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_faceswap/train/fake/') \n","    cv2.imwrite(filename, img)\n","  if count >= 800 and count < 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_faceswap/val/fake/')\n","    cv2.imwrite(filename, img)\n","  if count >= 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/Dataset_test/faceswap/')\n","    cv2.imwrite(filename, img)  \n","  count += 1"]},{"cell_type":"markdown","metadata":{"id":"z5tYTNffp0EB"},"source":["## NeuralTexture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YULVs9sspcUp"},"outputs":[],"source":["count = 0\n","deep_fake = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_neuraltexture_CROP'\n","for filename in os.listdir(deep_fake):\n","  file_path = deep_fake + '/' + filename\n","  img = cv2.imread(file_path)\n","  if count < 800:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_neuraltexture/train/fake/') \n","    cv2.imwrite(filename, img)\n","  if count >= 800 and count < 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_neuraltexture/val/fake/')\n","    cv2.imwrite(filename, img)\n","  if count >= 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/Dataset_test/neuraltexture/')\n","    cv2.imwrite(filename, img)  \n","  count += 1"]},{"cell_type":"markdown","source":["## Verify the number of file in the Folder\n"],"metadata":{"id":"_cTONDW24srX"}},{"cell_type":"code","source":["def count(dir):\n","  for fold in os.listdir(dir):\n","    dir_path=os.path.join(dir,fold)\n","    for fold2 in os.listdir(dir_path):\n","      l=[]\n","      for file in os.listdir(os.path.join(dir_path,fold2)):\n","        l.append(file)\n","      print('Dimension folder of '+ fold+' '+fold2)\n","      print(len(l))"],"metadata":{"id":"OLwFPJvs41K3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_deepfake')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_-nwXTT6Frj","executionInfo":{"status":"ok","timestamp":1673609951559,"user_tz":-60,"elapsed":563,"user":{"displayName":"niccolò piraino","userId":"01968820351330594054"}},"outputId":"bb851356-077e-4a4a-d84a-d9189a50b7c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimension folder of train real\n","800\n","Dimension folder of train fake\n","800\n","Dimension folder of val real\n","100\n","Dimension folder of val fake\n","100\n"]}]},{"cell_type":"code","source":["count('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_face2face')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3S-qVaar7hFC","executionInfo":{"status":"ok","timestamp":1673610054739,"user_tz":-60,"elapsed":7993,"user":{"displayName":"niccolò piraino","userId":"01968820351330594054"}},"outputId":"42ea261b-bac3-4570-8265-db1f6916ec7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimension folder of train real\n","800\n","Dimension folder of train fake\n","800\n","Dimension folder of val real\n","100\n","Dimension folder of val fake\n","100\n"]}]},{"cell_type":"code","source":["count('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_faceswap')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lcpIeZdR7lEs","executionInfo":{"status":"ok","timestamp":1673610054741,"user_tz":-60,"elapsed":27,"user":{"displayName":"niccolò piraino","userId":"01968820351330594054"}},"outputId":"d84c0ad0-83e8-4212-894d-03b4d5364c65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimension folder of train real\n","800\n","Dimension folder of train fake\n","800\n","Dimension folder of val real\n","100\n","Dimension folder of val fake\n","100\n"]}]},{"cell_type":"code","source":["count('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSet_neuraltexture')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-iwaBun77ojW","executionInfo":{"status":"ok","timestamp":1673610060328,"user_tz":-60,"elapsed":465,"user":{"displayName":"niccolò piraino","userId":"01968820351330594054"}},"outputId":"f5aa175c-38d6-4065-e1a3-d3839605808b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimension folder of train real\n","800\n","Dimension folder of train fake\n","800\n","Dimension folder of val real\n","100\n","Dimension folder of val fake\n","100\n"]}]},{"cell_type":"markdown","source":["# DATAVISION SET CREATION FOR TRAINING WITH ALL MANIPULATIONS"],"metadata":{"id":"eempNcGDr-09"}},{"cell_type":"markdown","source":["There, finally, we created a Dataset composed of all the manipulation for the binary task. We divided this Dataset in the usually three folder of training, validation and test (where the test folder used is the same composed with the function before)"],"metadata":{"id":"s6QYzn7NrCad"}},{"cell_type":"code","source":["count = 0\n","data_original = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_original_CROP'\n","for filename in os.listdir(data_original):\n","\n","  file_path = data_original + '/' + filename\n","  img = cv2.imread(file_path)\n","  if count < 800:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSetMix/train/real/') \n","    cv2.imwrite(filename, img)\n","    print(\"train: \",count)\n","  count += 1"],"metadata":{"id":"YH7nlk9AZCHP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","deep_fake = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_deepfake_CROP'\n","for filename in os.listdir(deep_fake):\n","  file_path = deep_fake + '/' + filename\n","  img = cv2.imread(file_path)\n","  if count < 200:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSetMix/train/fake/') \n","    cv2.imwrite(filename, img)\n","  if count >= 200 and count < 300:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSetMix/val/fake/')\n","    cv2.imwrite(filename, img)\n","\n","  count += 1"],"metadata":{"id":"5UL-UWXuZhNc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","face2face = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_face2face_CROP'\n","for filename in os.listdir(face2face):\n","  file_path = face2face + '/' + filename\n","  img = cv2.imread(file_path)\n","  if count < 200:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSetMix/train/fake/') \n","    cv2.imwrite(filename, img)\n","  if count >= 200 and count < 300:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSetMix/val/fake/')\n","    cv2.imwrite(filename, img)\n","  count += 1"],"metadata":{"id":"G-AKVPVsazFG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","deep_fake = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_faceswap_CROP'\n","for filename in os.listdir(deep_fake):\n","  file_path = deep_fake + '/' + filename\n","  img = cv2.imread(file_path)\n","  if count < 800:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSetMix/train/fake/') \n","    cv2.imwrite(filename, img)\n","  if count >= 800 and count < 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSetMix/val/fake/')\n","    cv2.imwrite(filename, img)\n","  count += 1"],"metadata":{"id":"WnB3XxeibGfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","deep_fake = '/content/drive/MyDrive/VISIOPE_PROJECT/DATACROP/data_neuraltexture_CROP'\n","for filename in os.listdir(deep_fake):\n","  file_path = deep_fake + '/' + filename\n","  img = cv2.imread(file_path)\n","  if count < 800:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSetMix/train/fake/') \n","    cv2.imwrite(filename, img)\n","  if count >= 800 and count < 900:\n","    os.chdir('/content/drive/MyDrive/VISIOPE_PROJECT/DataVisionSetMix/val/fake/')\n","    cv2.imwrite(filename, img)\n","  count += 1"],"metadata":{"id":"YSqcbyndbKNT"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["V5dNTgsP0BKL"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}